// NAILA AI Streaming Protocol
// Version: 1.0.0
// Owner: NAILA Core Team
//
// This protocol defines the gRPC interface between the Hub (Rust)
// and AI Server (Python) for real-time voice conversations.
//
// Design Principles:
// - Optimized for streaming STT/TTS with sub-600ms latency target
// - Flat message structure for minimal serialization overhead
// - Zero-copy where possible (audio bytes passed through)
// - AI-friendly chunking (200-500ms audio frames for Whisper)
//
// Changelog:
// - 1.0.0: Initial audio streaming protocol

syntax = "proto3";
package naila;

option optimize_for = SPEED;

// =============================================================================
// Service Definition
// =============================================================================

service NailaAI {
  // Bidirectional streaming for real-time voice conversations.
  //
  // Flow:
  // 1. Client opens stream, sends AudioInput with SPEECH_EVENT_START
  //    - First message MUST include session_config fields
  // 2. Client streams audio chunks with SPEECH_EVENT_CONTINUE (200-500ms each)
  // 3. Client sends SPEECH_EVENT_END when VAD detects silence
  // 4. Server streams back AudioOutput with TTS audio chunks
  // 5. Server sends is_final=true on last chunk
  // 6. If user interrupts (barge-in), client sends SPEECH_EVENT_INTERRUPT
  //
  // The stream remains open for the conversation session. Multiple
  // utterances can flow through a single stream.
  rpc StreamConversation(stream AudioInput) returns (stream AudioOutput);

  // Server status and capability check.
  // Call before opening streams to verify readiness and discover capabilities.
  // Also useful for monitoring dashboards and health checks.
  //
  // Note: For standard health checks, also implement grpc.health.v1.Health
  // This RPC provides NAILA-specific status beyond simple health.
  rpc GetStatus(StatusRequest) returns (StatusResponse);

  // TODO: Video frame analysis (Phase 4)
  // rpc AnalyzeFrame(FrameInput) returns (FrameAnalysis);

  // TODO: Context updates - presence, movement, device events (Phase 4)
  // rpc UpdateContext(ContextEvent) returns (Ack);
}

// =============================================================================
// Audio Input (Hub -> AI Server)
// =============================================================================

// Audio chunk from a device, pre-filtered by VAD in Hub.
// Only speech audio reaches the AI Server - silence is discarded upstream.
message AudioInput {
  // --- Routing (required on every message) ---

  // Unique device identifier (e.g., "pi_living_room_01")
  // Used for response routing and per-device context
  string device_id = 1;

  // Room identifier for multi-room awareness (e.g., "living_room")
  // Enables cross-room context and audio routing
  string room_id = 2;

  // --- Session Tracking ---

  // Unique conversation identifier for log correlation and context
  // Generated by the Hub, format: "conv_{uuid}" or "conv_{timestamp}_{device}"
  // MUST be set on every message; same ID for entire conversation session
  string conversation_id = 3;

  // --- Audio Data ---

  // Audio payload - prefer opus passthrough to avoid transcoding latency
  // PCM: Raw audio, 16-bit signed little-endian, mono
  // Opus: WebRTC default codec, preserves quality, smaller wire size
  oneof audio {
    bytes audio_pcm = 4;   // Raw PCM samples
    bytes audio_opus = 5;  // Opus-encoded (WebRTC passthrough)
  }

  // Codec indicator - must match the oneof field used
  AudioCodec codec = 6;

  // Sample rate in Hz (typically 16000 for STT, 48000 for WebRTC)
  // AI Server resamples if needed for model compatibility
  uint32 sample_rate = 7;

  // Duration of this audio chunk in milliseconds
  // Critical for STT buffering: 200-500ms optimal for streaming Whisper
  // Too small (<100ms): overhead dominates, poor transcription
  // Too large (>1000ms): latency increases, user experience degrades
  uint32 chunk_duration_ms = 8;

  // --- Timing & Ordering ---

  // Unix timestamp in milliseconds when audio was captured
  // Used for latency measurement and chunk ordering
  uint64 timestamp_ms = 9;

  // Monotonically increasing sequence number per stream
  // Enables detection of dropped/reordered chunks
  uint32 sequence_num = 10;

  // --- Speech State ---

  // Current state of the speech segment
  // Controls STT buffering and response generation
  SpeechEvent event = 11;

  // --- Context (populated on SPEECH_EVENT_START, optional on CONTINUE) ---

  // Identified speaker if known (e.g., "user_alice")
  // Enables personalized responses and memory retrieval
  string user_id = 12;

  // Other users detected in the room (from vision/presence)
  // Enables context-aware responses ("Tell everyone..." vs direct)
  repeated string users_present = 13;

  // Current activity context (e.g., "cooking", "watching_tv")
  // Helps LLM generate contextually appropriate responses
  string active_task = 14;

  // --- Session Config (MUST be set on first message with SPEECH_EVENT_START) ---

  // Client capabilities and preferences for this session
  // Only read from the first message; ignored on subsequent messages
  SessionConfig session_config = 15;

  // Reserved for future use
  reserved 16 to 19;
  reserved "wake_word_confidence", "noise_level_db";
}

// Session configuration sent on first message of a stream
// Allows client to declare capabilities and preferences
message SessionConfig {
  // Client identifier and version for debugging
  string client_id = 1;           // e.g., "hub-rust"
  string client_version = 2;      // e.g., "1.0.0"

  // Preferred output sample rate for TTS audio
  // Server will try to match, falls back to native TTS rate if unsupported
  uint32 preferred_output_sample_rate = 3;  // e.g., 22050, 24000, 48000

  // Codecs the client can decode for TTS playback
  // Server will use first supported codec from this list
  repeated AudioCodec supported_output_codecs = 4;

  // Enable partial STT results (incremental transcription)
  // If false, server only sends final transcription
  bool enable_partial_stt = 5;

  // Enable transcript in AudioOutput (text of what's being spoken)
  // Useful for captions, can disable to save bandwidth
  bool enable_transcript = 6;

  // Maximum concurrent conversations this client expects
  // Helps server allocate resources; 0 = no limit / server decides
  uint32 max_concurrent_conversations = 7;

  // Language hint for STT (BCP-47 format, e.g., "en-US", "es-MX")
  // Empty = auto-detect
  string language_hint = 8;
}

// Audio encoding format
enum AudioCodec {
  AUDIO_CODEC_UNKNOWN = 0;        // Invalid/unset - reject message
  AUDIO_CODEC_PCM_S16LE = 1;      // Raw PCM, signed 16-bit little-endian, mono
  AUDIO_CODEC_OPUS = 2;           // Opus codec (WebRTC default)
}

// Speech segment state machine
// State transitions: START -> CONTINUE* -> END
//                    Any state -> INTERRUPT (barge-in)
enum SpeechEvent {
  SPEECH_EVENT_UNKNOWN = 0;       // Invalid/unset - reject message
  SPEECH_EVENT_START = 1;         // First chunk of utterance (VAD triggered)
  SPEECH_EVENT_CONTINUE = 2;      // Ongoing speech
  SPEECH_EVENT_END = 3;           // End of utterance (VAD silence detected)
  SPEECH_EVENT_INTERRUPT = 4;     // User interrupted TTS playback (barge-in)
}

// =============================================================================
// Audio Output (AI Server -> Hub)
// =============================================================================

// TTS audio chunk or control message from AI Server.
// Streamed incrementally to minimize time-to-first-audio.
message AudioOutput {
  // --- Routing ---

  // Target device for audio playback
  string device_id = 1;

  // Target room (may differ from input for cross-room responses)
  string room_id = 2;

  // --- Session Tracking ---

  // Conversation ID echoed from input for correlation
  string conversation_id = 3;

  // --- Audio Data ---

  // TTS audio chunk - PCM format for universal device compatibility
  // Empty bytes with is_final=false indicates a control-only message
  bytes audio_pcm = 4;

  // Sample rate of output audio (typically 22050 or 24000 for TTS)
  uint32 sample_rate = 5;

  // Sequence number for chunk ordering and drop detection
  uint32 sequence_num = 6;

  // --- Stream Control ---

  // True on the last audio chunk of the response
  // Hub should flush audio buffer and prepare for next input
  bool is_final = 7;

  // --- Incremental Feedback (for UI/debugging) ---

  // Text being spoken in this chunk (for captions/logging)
  // Only populated if session_config.enable_transcript = true
  string transcript = 8;

  // Partial STT result (incremental transcription before END)
  // Only populated if session_config.enable_partial_stt = true
  // Enables "live typing" UI feedback
  string partial_stt = 9;

  // STT confidence score [0.0, 1.0] on final transcription
  float stt_confidence = 10;

  // Final STT transcription of what the user said
  // Populated on first AudioOutput after processing completes
  string final_stt = 11;

  // --- AI Requests (piggyback on response stream) ---
  // Zero wire cost when not set (default NONE)

  // Type of request AI is making to Hub
  AIRequestType request_type = 12;

  // Target room for the request (e.g., "get frame from kitchen")
  string request_room_id = 13;

  // Target device for the request (optional, room-level default)
  string request_device_id = 14;

  // Request-specific payload (command name, query, etc.)
  string request_payload = 15;

  // --- Error Handling ---

  // Error code if processing failed
  ErrorCode error_code = 16;

  // Human-readable error message for logging/debugging
  string error_message = 17;

  // Reserved for future use
  reserved 18 to 20;
  reserved "emotion", "suggested_expression";
}

// AI-initiated requests to Hub
// Enables AI to gather additional context or route responses
enum AIRequestType {
  AI_REQUEST_NONE = 0;            // No request (default)
  AI_REQUEST_FRAME = 1;           // Request camera frame from room/device
  AI_REQUEST_ROUTE_AUDIO = 2;     // Route this response to different room
  AI_REQUEST_DEVICE_COMMAND = 3;  // Send command to device (via MQTT)
  AI_REQUEST_CONTEXT_QUERY = 4;   // Query context store (where is user?)
}

// Typed error codes for programmatic handling
enum ErrorCode {
  ERROR_NONE = 0;                 // No error
  ERROR_INVALID_AUDIO = 1;        // Malformed or corrupt audio data
  ERROR_CODEC_UNSUPPORTED = 2;    // Unsupported audio codec
  ERROR_STT_FAILED = 3;           // Speech-to-text processing failed
  ERROR_LLM_FAILED = 4;           // Language model inference failed
  ERROR_TTS_FAILED = 5;           // Text-to-speech synthesis failed
  ERROR_CONTEXT_UNAVAILABLE = 6;  // Context store unreachable
  ERROR_RATE_LIMITED = 7;         // Too many requests, backoff
  ERROR_SESSION_INVALID = 8;      // Invalid or expired session
  ERROR_CONFIG_INVALID = 9;       // Invalid session config
  ERROR_INTERNAL = 99;            // Unspecified internal error
}

// =============================================================================
// Status & Health
// =============================================================================

// Request for server status (empty for now, extensible)
message StatusRequest {
  // Include model details in response (may be slower)
  bool include_model_info = 1;

  // Include resource utilization metrics
  bool include_metrics = 2;
}

// Server status and capabilities
message StatusResponse {
  // --- Health ---

  // Overall server health
  ServerHealth health = 1;

  // Server version
  string server_version = 2;

  // Server uptime in seconds
  uint64 uptime_seconds = 3;

  // --- Capabilities ---

  // Audio codecs the server can process (input)
  repeated AudioCodec supported_input_codecs = 4;

  // Audio codecs the server can produce (output)
  repeated AudioCodec supported_output_codecs = 5;

  // Sample rates the server supports for input
  repeated uint32 supported_input_sample_rates = 6;

  // Sample rates the server can produce for output
  repeated uint32 supported_output_sample_rates = 7;

  // Maximum concurrent streams the server can handle
  // 0 = unlimited / not specified
  uint32 max_concurrent_streams = 8;

  // Languages supported for STT (BCP-47 format)
  repeated string supported_languages = 9;

  // --- Model Info (if requested) ---

  // Active AI models
  ModelInfo stt_model = 10;
  ModelInfo llm_model = 11;
  ModelInfo tts_model = 12;

  // --- Metrics (if requested) ---

  // Current resource utilization
  ServerMetrics metrics = 13;

  // --- Component Health ---

  // Individual component status
  repeated ComponentHealth components = 14;
}

enum ServerHealth {
  SERVER_HEALTH_UNKNOWN = 0;
  SERVER_HEALTH_HEALTHY = 1;      // All systems operational
  SERVER_HEALTH_DEGRADED = 2;     // Some features limited
  SERVER_HEALTH_UNHEALTHY = 3;    // Not accepting requests
}

// Information about an AI model
message ModelInfo {
  // Model identifier (e.g., "whisper-large-v3", "llama-3.1-8b")
  string model_id = 1;

  // Model version or checkpoint
  string version = 2;

  // Whether model is loaded and ready
  bool loaded = 3;

  // Device model is running on (e.g., "cuda:0", "cpu")
  string device = 4;

  // Model-specific metadata
  map<string, string> metadata = 5;
}

// Server resource metrics
message ServerMetrics {
  // Active conversation streams
  uint32 active_streams = 1;

  // CPU utilization [0.0, 1.0]
  float cpu_utilization = 2;

  // Memory utilization [0.0, 1.0]
  float memory_utilization = 3;

  // GPU utilization [0.0, 1.0] (0 if no GPU)
  float gpu_utilization = 4;

  // GPU memory utilization [0.0, 1.0] (0 if no GPU)
  float gpu_memory_utilization = 5;

  // Average response latency in milliseconds (rolling window)
  uint32 avg_latency_ms = 6;

  // Requests processed in last minute
  uint32 requests_per_minute = 7;
}

// Individual component health status
message ComponentHealth {
  // Component name (e.g., "stt", "llm", "tts", "context_store")
  string name = 1;

  // Component health status
  ServerHealth health = 2;

  // Optional status message
  string message = 3;

  // Last successful operation timestamp (unix ms)
  uint64 last_success_ms = 4;
}

// =============================================================================
// Placeholders - Video & Context (Phase 4)
// =============================================================================

// message FrameInput {
//   string device_id = 1;
//   string room_id = 2;
//   string conversation_id = 3;
//   bytes frame_jpeg = 4;
//   uint32 width = 5;
//   uint32 height = 6;
//   uint64 timestamp_ms = 7;
//   FrameTrigger trigger = 8;  // Why was this frame sent?
// }

// enum FrameTrigger {
//   FRAME_TRIGGER_UNKNOWN = 0;
//   FRAME_TRIGGER_MOTION = 1;      // Motion detected
//   FRAME_TRIGGER_SCHEDULED = 2;   // Periodic capture
//   FRAME_TRIGGER_REQUESTED = 3;   // AI requested frame
//   FRAME_TRIGGER_PERSON = 4;      // Person entered room
// }

// message FrameAnalysis {
//   string conversation_id = 1;
//   repeated string users_detected = 2;
//   string scene_description = 3;
//   uint32 processing_ms = 4;
// }

// message ContextEvent {
//   string room_id = 1;
//   string user_id = 2;
//   string event_type = 3;       // "entered", "left", "activity_changed"
//   string details = 4;
//   uint64 timestamp_ms = 5;
// }

// message Ack {
//   bool success = 1;
//   string message = 2;
// }
