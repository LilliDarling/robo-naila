# TTS (Text-to-Speech) Service Implementation Plan

**Status**: Not Started
**Priority**: P1 - High
**Owner**: AI-Server Team
**Created**: 2025-11-04
**Model**: Piper TTS - Lessac Voice (en_US-lessac-medium)

---

## Overview

Implement the TTS (Text-to-Speech) service to synthesize natural-sounding speech from text responses generated by the NAILA AI-Server. This service will enable the robot to speak responses back to users, completing the voice interaction loop (STT → LLM → TTS).

---

## Current State Analysis

### What Exists
- ✅ Model downloaded: `models/tts/en/en_US/lessac/medium/en_US-lessac-medium.onnx` (61MB)
- ✅ Model config: `en_US-lessac-medium.onnx.json` with phoneme mappings
- ✅ Hardware detection: `config/hardware_config.py`
- ✅ MQTT topics defined: `naila/ai/responses/audio`
- ✅ LLM service generating text responses
- ✅ Response generator creating responses

### What's Missing
- ❌ TTS service implementation
- ❌ Text preprocessing (normalization, SSML support)
- ❌ Audio post-processing (volume, speed adjustment)
- ❌ Integration with response pipeline
- ❌ MQTT audio response publishing
- ❌ Streaming vs batch synthesis
- ❌ Performance monitoring for synthesis

### Dependencies Required
```
piper-tts>=1.3.0           # Official Piper TTS library
# OR
onnxruntime>=1.16.0        # For direct ONNX inference
espeak-ng                  # Phoneme conversion (system package)
```

**Recommendation**: Use `piper-tts` for easiest integration, or `onnxruntime` for more control.

---

## Implementation Plan

### Phase 1: Service Foundation (Create the TTS Service Class)

**Goal**: Create the basic TTS service structure with model loading

**Files to Create**:
- `services/tts.py`
- `config/tts.py`

**Implementation Details**:

1. **Create `TTSService` class** with:
   - `__init__()` - Initialize configuration
   - `async load_model()` - Load the Piper ONNX model
   - `async synthesize()` - Convert text to audio bytes
   - `async synthesize_to_file()` - Save audio to file
   - `unload_model()` - Clean up resources
   - `is_loaded()` - Check if model is ready

2. **Configuration Parameters** (`config/tts.py`):
   - Model path from `.env`
   - Voice (lessac, default, etc.)
   - Sample rate (22050 Hz for Piper)
   - Speaker ID (if multi-speaker model)
   - Inference parameters (noise_scale, length_scale, noise_w)
   - Output format (WAV, MP3, OGG)
   - Audio quality settings

3. **Hardware Optimization**:
   - Use `hardware_config.py` to determine optimal settings
   - Enable GPU acceleration if available (ONNX Runtime supports CUDA)
   - Set appropriate thread count for CPU inference
   - Configure execution provider (CPU, CUDA, TensorRT)

4. **Text Preprocessing**:
   - Normalize text (numbers → words, abbreviations)
   - Handle punctuation for natural prosody
   - Support SSML tags for advanced control
   - Clean special characters
   - Split long text into manageable chunks

**Key Considerations**:
- Piper uses ONNX format for fast inference
- eSpeak-ng required for phoneme conversion
- Model loading takes 1-2 seconds
- Synthesis time ~0.1-0.3x real-time (very fast)
- Sample rate 22050 Hz (high quality)

**Success Criteria**:
- [ ] TTSService class created with all methods
- [ ] Model loads successfully without errors
- [ ] Can synthesize speech from text
- [ ] Audio quality is clear and natural
- [ ] Hardware acceleration configured

---

### Phase 2: Audio Synthesis Engine

**Goal**: Implement high-quality text-to-speech synthesis

**Implementation Details**:

1. **Synthesis Method**:
   ```python
   async def synthesize(
       text: str,
       speaker_id: int = 0,
       length_scale: float = 1.0,
       noise_scale: float = 0.667,
       noise_w: float = 0.8,
       output_format: str = "wav"
   ) -> AudioData:
       # Preprocess text
       # Convert text to phonemes (eSpeak)
       # Run ONNX inference
       # Post-process audio
       # Encode to output format
       return audio_data
   ```

2. **Audio Data Structure**:
   ```python
   @dataclass
   class AudioData:
       audio_bytes: bytes           # Raw audio data
       sample_rate: int             # 22050 Hz
       format: str                  # "wav", "mp3", "ogg"
       duration_ms: int             # Audio length
       synthesis_time_ms: int       # Processing time
       text: str                    # Original text
       phonemes: str                # IPA phonemes
   ```

3. **Text Normalization**:
   - Numbers: "123" → "one hundred twenty-three"
   - Dates: "11/04/2025" → "November fourth, twenty twenty-five"
   - Times: "3:30 PM" → "three thirty PM"
   - Abbreviations: "Dr." → "Doctor"
   - URLs/emails: Handle or skip
   - Currency: "$50" → "fifty dollars"

4. **Prosody Control**:
   - Speaking rate (length_scale): 0.5 = 2x faster, 2.0 = 2x slower
   - Pitch variation (noise_scale): Controls naturalness
   - Energy variation (noise_w): Controls expressiveness
   - Sentence-level adjustments based on punctuation

5. **Audio Post-Processing**:
   - Normalize volume levels
   - Apply fade in/out
   - Remove clicks/pops
   - Resample if needed (e.g., 22050 → 16000 Hz for robot speaker)
   - Compress for MQTT (MP3/OGG)

**Success Criteria**:
- [ ] Text synthesized clearly and naturally
- [ ] Numbers and dates spoken correctly
- [ ] Prosody sounds natural (not robotic)
- [ ] Audio quality is high
- [ ] Processing time fast (<0.5x real-time)

---

### Phase 3: Output Format Handling

**Goal**: Support multiple audio output formats for different use cases

**Implementation Details**:

1. **Supported Output Formats**:
   - **WAV** (uncompressed, highest quality, large file)
   - **MP3** (compressed, good quality, smaller file)
   - **OGG** (open format, good compression)
   - **RAW PCM** (for direct speaker output)

2. **Format Encoding**:
   ```python
   def encode_audio(
       audio_samples: np.ndarray,
       sample_rate: int,
       format: str
   ) -> bytes:
       if format == "wav":
           return encode_wav(audio_samples, sample_rate)
       elif format == "mp3":
           return encode_mp3(audio_samples, sample_rate, bitrate=128)
       elif format == "ogg":
           return encode_ogg(audio_samples, sample_rate, quality=6)
       else:
           return audio_samples.tobytes()  # Raw PCM
   ```

3. **MQTT Audio Message Format**:
   ```json
   {
       "device_id": "naila_001",
       "timestamp": "2025-11-04T12:34:56Z",
       "audio_data": "<base64_encoded_audio>",
       "format": "mp3",
       "sample_rate": 22050,
       "duration_ms": 3200,
       "text": "Hello! How can I help you today?",
       "metadata": {
           "voice": "lessac",
           "language": "en_US",
           "synthesis_time_ms": 450
       }
   }
   ```

4. **Streaming Support** (Future):
   - Chunk long text into sentences
   - Synthesize and stream incrementally
   - Reduce perceived latency

**Success Criteria**:
- [ ] Multiple output formats supported
- [ ] Format encoding working correctly
- [ ] MQTT messages properly formatted
- [ ] Compression reduces file size effectively
- [ ] Audio quality maintained after encoding

---

### Phase 4: Integration with Response Pipeline

**Goal**: Connect TTS service to response generator and MQTT publishing

**Files to Modify**:
- `agents/response_generator.py`
- `mqtt/handlers/ai_handlers.py`
- `agents/orchestrator.py`

**Implementation Details**:

1. **Add TTS to Response Generator**:
   ```python
   # In response_generator.py
   async def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
       # Generate text response (existing)
       response_text = await self._generate_response(...)

       # Synthesize to audio (NEW)
       if self.tts_service and self.tts_service.is_ready:
           try:
               audio_data = await self.tts_service.synthesize(
                   response_text,
                   output_format="mp3"
               )
               state["response_audio"] = audio_data
           except Exception as e:
               logger.warning(f"TTS synthesis failed: {e}")

       return state
   ```

2. **Publish Audio Response**:
   ```python
   # In ai_handlers.py or orchestrator.py
   async def _publish_response(self, state: Dict):
       # Publish text response (existing)
       self.mqtt_service.publish_ai_response(
           device_id, state["response_text"]
       )

       # Publish audio response (NEW)
       if "response_audio" in state:
           audio_message = {
               "audio_data": base64.b64encode(state["response_audio"].audio_bytes),
               "format": state["response_audio"].format,
               "sample_rate": state["response_audio"].sample_rate,
               "duration_ms": state["response_audio"].duration_ms,
               "text": state["response_text"]
           }
           self.mqtt_service.publish(
               f"naila/ai/responses/audio",
               audio_message,
               qos=1
           )
   ```

3. **Integration Flow**:
   ```
   1. User query received (text or audio)
      ↓
   2. Orchestrator processes through pipeline
      ↓
   3. LLM generates text response
      ↓
   4. Response Generator receives text
      ↓
   5. TTS Service synthesizes audio (NEW)
      ↓
   6. Both text and audio sent via MQTT
      ↓
   7. Robot speaks audio response
   ```

4. **Configuration Options**:
   - Enable/disable TTS per request
   - Choose voice/speaker dynamically
   - Adjust speaking rate for urgency
   - Skip TTS for very short responses

**Success Criteria**:
- [ ] TTS integrated into response pipeline
- [ ] Audio responses published to MQTT
- [ ] Text and audio sent together
- [ ] Configuration options working
- [ ] End-to-end voice interaction complete

---

### Phase 5: Server Lifecycle Integration

**Goal**: Load TTS during server startup, handle gracefully during shutdown

**Files to Modify**:
- `server/lifecycle.py`
- `server/naila_server.py`

**Implementation Details**:

1. **Add TTS Loading Phase**:
   ```python
   # In lifecycle.py _load_ai_models()
   async def _load_ai_models(self):
       # Load LLM (existing)
       if self.llm_service:
           await self.llm_service.load_model()

       # Load STT (existing)
       if self.stt_service:
           await self.stt_service.load_model()

       # Load TTS (NEW)
       if self.tts_service:
           logger.info("Loading TTS model...")
           success = await self.tts_service.load_model()
           if success:
               logger.info(f"TTS model loaded: {self.tts_service.voice_name}")
               self.protocol_handlers.set_tts_service(self.tts_service)
           else:
               logger.warning("TTS model failed to load - audio responses disabled")
   ```

2. **Initialization Sequence**:
   ```
   Phase 1: Initialize configuration
   Phase 2: Load AI models
       - Load LLM model
       - Load STT model
       - Load TTS model (NEW)
   Phase 3: Register protocol handlers
   Phase 4: Start MQTT service
   Phase 5: Start health monitoring
   ```

3. **Pass TTS to Components**:
   - Store TTS service in `NailaAIServer`
   - Pass to `ResponseGenerator` for audio synthesis
   - Pass to `AIHandlers` for publishing
   - Make available via dependency injection

4. **Shutdown Handling**:
   - Unload TTS model during graceful shutdown
   - Free memory resources
   - Cancel any in-flight synthesis

**Success Criteria**:
- [ ] TTS loads during server startup
- [ ] Loading status visible in logs
- [ ] TTS service accessible to components
- [ ] Graceful shutdown unloads TTS properly
- [ ] Server handles TTS load failures without crashing

---

### Phase 6: Performance Optimization & Monitoring

**Goal**: Ensure TTS performs efficiently and track key metrics

**Implementation Details**:

1. **Performance Monitoring**:
   - Track synthesis time per request
   - Monitor real-time factor (RTF)
   - Log audio duration
   - Track model memory usage
   - Count successful vs failed syntheses

2. **Add to Health Monitoring**:
   - Include TTS status in health checks
   - Report synthesis metrics in `naila/ai/metrics` topic
   - Alert if synthesis times exceed thresholds

3. **Optimization Techniques**:
   - Cache common phrases (e.g., greetings)
   - Batch synthesis for multiple sentences
   - Preload model during startup
   - Use CPU for TTS (already fast enough)
   - Compress audio for MQTT transmission

4. **Logging & Debugging**:
   - Log text being synthesized (debug mode)
   - Log synthesis parameters
   - Track audio quality metrics
   - Log any synthesis errors

**Metrics to Track**:
```python
{
    "tts_status": "loaded" | "unloaded" | "error",
    "model_name": "en_US-lessac-medium",
    "voice": "lessac",
    "synthesis_time_ms": 320,
    "audio_duration_ms": 3200,
    "real_time_factor": 0.10,  # Much faster than real-time
    "text_length": 45,
    "output_format": "mp3",
    "file_size_bytes": 51200,
    "language": "en_US"
}
```

**Success Criteria**:
- [ ] Synthesis metrics tracked and logged
- [ ] Performance data included in health metrics
- [ ] RTF << 1.0 (much faster than real-time)
- [ ] No memory leaks during extended operation

---

### Phase 7: Advanced Features & Quality

**Goal**: Enhance TTS with advanced features for better user experience

**Implementation Details**:

1. **SSML Support** (Optional):
   ```xml
   <speak>
       Hello! <break time="500ms"/>
       The temperature is <prosody rate="slow">twenty-five</prosody> degrees.
       <emphasis level="strong">This is important!</emphasis>
   </speak>
   ```
   - Parse SSML tags
   - Apply prosody controls
   - Handle breaks and pauses
   - Support emphasis

2. **Emotion/Tone Control** (Future):
   - Adjust pitch for different emotions
   - Vary speaking rate for urgency
   - Add prosody for questions vs statements
   - Personality-aware synthesis

3. **Multi-Voice Support** (Future):
   - Load multiple voice models
   - Switch voices dynamically
   - Different voices for different contexts
   - Character voices for stories

4. **Audio Effects** (Optional):
   - Reverb for spatial audio
   - EQ for speaker optimization
   - Noise gate for clean output
   - Limiter for consistent volume

**Success Criteria**:
- [ ] Basic SSML tags supported
- [ ] Emotion/tone control working
- [ ] Audio effects applied correctly
- [ ] Voice switching implemented

---

### Phase 8: Testing & Validation

**Goal**: Ensure TTS service works correctly in all scenarios

**Test Cases**:

1. **Unit Tests** (`tests/unit/test_tts_service.py`):
   - [ ] Model loading succeeds
   - [ ] Synthesize simple text
   - [ ] Text normalization works
   - [ ] Handle invalid input gracefully
   - [ ] Format encoding works
   - [ ] Unload model properly

2. **Integration Tests** (`tests/integration/test_tts_integration.py`):
   - [ ] TTS loads during server startup
   - [ ] Response generator uses TTS
   - [ ] Audio messages published via MQTT
   - [ ] End-to-end text-to-audio flow
   - [ ] Fallback works when TTS unavailable

3. **Audio Quality Tests**:
   - [ ] Short text (1-2 words)
   - [ ] Medium text (1-2 sentences)
   - [ ] Long text (paragraph)
   - [ ] Numbers and dates
   - [ ] Punctuation (questions, exclamations)
   - [ ] Special characters
   - [ ] Very long text (chunking)
   - [ ] Empty text (error handling)

4. **Manual Testing**:
   - [ ] Listen to synthesized audio
   - [ ] Verify naturalness and clarity
   - [ ] Check pronunciation of words
   - [ ] Test different speaking rates
   - [ ] Verify MQTT audio playback on robot

**Success Criteria**:
- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] Audio quality meets expectations
- [ ] Processing time acceptable (RTF < 0.5)

---

## Technical Architecture

### Component Diagram
```
┌─────────────────────────────────────────────────────────────┐
│                      NAILA AI Server                         │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│                          ┌──────────────────┐               │
│                          │  Orchestrator    │               │
│                          └────────┬─────────┘               │
│                                   │                          │
│                                   ▼                          │
│                          ┌──────────────────┐               │
│                          │  LLM Service     │               │
│                          │  (Text Gen)      │               │
│                          └────────┬─────────┘               │
│                                   │                          │
│                                   ▼                          │
│                          ┌──────────────────┐               │
│                          │ Response         │               │
│                          │ Generator        │               │
│                          └────────┬─────────┘               │
│                                   │                          │
│                                   │ Text Response            │
│                                   ▼                          │
│                          ┌──────────────────┐               │
│                          │  TTS Service     │◀──┐           │
│                          │  (NEW)           │   │           │
│                          └────────┬─────────┘   │           │
│                                   │             │           │
│                                   ▼             │           │
│                          ┌──────────────────┐   │           │
│                          │ Piper TTS        │   │           │
│                          │ (ONNX Runtime)   │   │           │
│                          └────────┬─────────┘   │           │
│                                   │             │           │
│                                   ▼             │           │
│                          ┌──────────────────┐   │           │
│                          │ eSpeak-ng        │   │           │
│                          │ (Phonemes)       │   │           │
│                          └────────┬─────────┘   │           │
│                                   │             │           │
│                                   ▼             │           │
│                          ┌──────────────────┐   │           │
│                          │ Audio Encoder    │   │           │
│                          │ (WAV/MP3/OGG)    │   │           │
│                          └────────┬─────────┘   │           │
│                                   │             │           │
│                                   │ Audio Bytes │           │
│                                   ▼             │           │
│  ┌──────────────┐         ┌──────────────────┐ │           │
│  │  MQTT Layer  │◀────────│  AI Handlers     │─┘           │
│  │              │         └──────────────────┘             │
│  │  Audio Topic │                                           │
│  └──────────────┘                                           │
│         │                                                    │
│         ▼                                                    │
│    Robot Speaker                                             │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

### Data Flow
```
1. User query received (audio/text)
   ↓
2. STT transcribes to text (if audio)
   ↓
3. Orchestrator processes query
   ↓
4. LLM generates text response
   ↓
5. Response Generator receives text
   ↓
6. TTS Service:
   - Normalize text (numbers, dates, etc.)
   - Convert text to phonemes (eSpeak-ng)
   - Run Piper ONNX inference
   - Generate audio samples (22050 Hz)
   - Apply post-processing
   - Encode to MP3/OGG
   ↓
7. AI Handler publishes to MQTT:
   - Text response: naila/ai/responses/text
   - Audio response: naila/ai/responses/audio
   ↓
8. Robot receives and plays audio
```

---

## Configuration

### Environment Variables (`.env`)
```bash
# TTS Configuration
TTS_MODEL_PATH=models/tts/en/en_US/lessac/medium/en_US-lessac-medium.onnx
TTS_VOICE=lessac
TTS_LANGUAGE=en_US
TTS_SPEAKER_ID=0

# Synthesis Parameters
TTS_SAMPLE_RATE=22050
TTS_LENGTH_SCALE=1.0      # Speaking rate (1.0 = normal)
TTS_NOISE_SCALE=0.667     # Pitch variation
TTS_NOISE_W=0.8           # Energy variation

# Output Settings
TTS_OUTPUT_FORMAT=mp3     # wav, mp3, ogg
TTS_MP3_BITRATE=128       # kbps
TTS_OGG_QUALITY=6         # 0-10

# Performance
TTS_ENABLE_GPU=false      # TTS is fast enough on CPU
TTS_THREADS=2
TTS_CACHE_COMMON_PHRASES=true

# Text Processing
TTS_NORMALIZE_NUMBERS=true
TTS_NORMALIZE_DATES=true
TTS_MAX_TEXT_LENGTH=500   # Characters
```

### Configuration Module (`config/tts.py`)
```python
"""TTS Service Configuration Constants"""

import os
from pathlib import Path

# Model Configuration
MODEL_PATH = os.getenv(
    "TTS_MODEL_PATH",
    "models/tts/en/en_US/lessac/medium/en_US-lessac-medium.onnx"
)
VOICE = os.getenv("TTS_VOICE", "lessac")
LANGUAGE = os.getenv("TTS_LANGUAGE", "en_US")
SPEAKER_ID = int(os.getenv("TTS_SPEAKER_ID", "0"))

# Synthesis Parameters
SAMPLE_RATE = int(os.getenv("TTS_SAMPLE_RATE", "22050"))
LENGTH_SCALE = float(os.getenv("TTS_LENGTH_SCALE", "1.0"))
NOISE_SCALE = float(os.getenv("TTS_NOISE_SCALE", "0.667"))
NOISE_W = float(os.getenv("TTS_NOISE_W", "0.8"))

# Output Settings
OUTPUT_FORMAT = os.getenv("TTS_OUTPUT_FORMAT", "mp3")
MP3_BITRATE = int(os.getenv("TTS_MP3_BITRATE", "128"))
OGG_QUALITY = int(os.getenv("TTS_OGG_QUALITY", "6"))

# Performance
ENABLE_GPU = os.getenv("TTS_ENABLE_GPU", "false").lower() == "true"
THREADS = int(os.getenv("TTS_THREADS", "2"))
CACHE_COMMON_PHRASES = os.getenv("TTS_CACHE_COMMON_PHRASES", "true").lower() == "true"

# Text Processing
NORMALIZE_NUMBERS = os.getenv("TTS_NORMALIZE_NUMBERS", "true").lower() == "true"
NORMALIZE_DATES = os.getenv("TTS_NORMALIZE_DATES", "true").lower() == "true"
MAX_TEXT_LENGTH = int(os.getenv("TTS_MAX_TEXT_LENGTH", "500"))

# Common Phrases to Cache
COMMON_PHRASES = [
    "Hello",
    "Goodbye",
    "How can I help you?",
    "I don't understand",
    "Could you repeat that?",
    "One moment please",
    "I'm sorry",
    "Thank you",
    "You're welcome"
]

# Logging
LOG_SYNTHESES = os.getenv("TTS_LOG_SYNTHESES", "true").lower() == "true"
LOG_PERFORMANCE_METRICS = True
```

---

## Risks & Mitigations

### Risk 1: Model Loading Time
- **Risk**: 1-2 second startup delay
- **Impact**: Slightly longer server startup
- **Mitigation**: Load in parallel with other models, show progress

### Risk 2: Audio Quality
- **Risk**: Robotic or unnatural speech
- **Impact**: Poor user experience
- **Mitigation**:
  - Use high-quality Lessac voice model
  - Tune synthesis parameters
  - Apply proper text normalization
  - Test and iterate on quality

### Risk 3: Synthesis Latency
- **Risk**: Delay between text generation and audio playback
- **Impact**: Perceived slowness
- **Mitigation**:
  - TTS is very fast (0.1-0.3x real-time)
  - Use streaming for long responses
  - Optimize network transmission (compression)

### Risk 4: Text Normalization Issues
- **Risk**: Numbers/dates pronounced incorrectly
- **Impact**: Confusing or incorrect speech
- **Mitigation**:
  - Implement robust normalization
  - Test common patterns
  - Handle edge cases
  - Allow manual overrides

### Risk 5: Network Bandwidth
- **Risk**: Large audio files over MQTT
- **Impact**: Slow transmission, network congestion
- **Mitigation**:
  - Use MP3/OGG compression
  - Adjust bitrate based on quality needs
  - Consider local TTS on robot (future)

---

## Success Metrics

### Functional Metrics
- [ ] Model loads successfully on server start
- [ ] Synthesizes clear, natural-sounding speech
- [ ] Handles text normalization correctly
- [ ] Integrates seamlessly with response pipeline
- [ ] Server stable during extended operation

### Performance Metrics
- Model load time: < 3 seconds
- Synthesis RTF: < 0.3 (3x faster than real-time)
- Average synthesis time: < 1 second for 3-second audio
- Memory usage: < 500MB
- No memory leaks over 24 hour operation

### Quality Metrics
- Speech naturalness: Subjectively "good"
- Pronunciation accuracy: > 95%
- Prosody appropriateness: Natural intonation
- Audio clarity: No artifacts or distortion

---

## Timeline & Milestones

### Milestone 1: Basic TTS Service (2-3 hours)
- Create TTSService class
- Implement model loading
- Basic text-to-speech working

### Milestone 2: Audio Quality (2-3 hours)
- Text normalization
- Prosody tuning
- Audio post-processing

### Milestone 3: Integration (2-3 hours)
- Integrate with response generator
- MQTT audio publishing
- Add to server lifecycle

### Milestone 4: Testing & Polish (2-3 hours)
- Write tests
- Performance monitoring
- Quality validation

**Total Estimated Time**: 8-12 hours

---

## Dependencies & Prerequisites

### Required
- [x] Piper TTS model downloaded
- [ ] `piper-tts` or `onnxruntime` package installed
- [ ] `espeak-ng` system package installed
- [x] Hardware detection working
- [x] MQTT infrastructure exists
- [x] Response generator exists

### System Dependencies
```bash
# Install eSpeak-ng (required for phoneme conversion)
sudo apt-get install espeak-ng

# Or on macOS
brew install espeak-ng
```

### Optional (Future Enhancements)
- [ ] GPU available for acceleration (not needed, CPU is fast)
- [ ] Streaming synthesis support
- [ ] Multiple voice models
- [ ] SSML support library

---

## Future Enhancements

### Short-term (Next Sprint)
1. **Phrase Caching**: Cache common phrases to reduce latency
2. **SSML Support**: Advanced prosody control
3. **Streaming Synthesis**: Real-time audio generation
4. **Volume Normalization**: Consistent audio levels

### Long-term (Future Sprints)
1. **Multi-Voice Support**: Switch between different voices
2. **Emotion Control**: Adjust prosody for emotions
3. **Custom Voice Training**: Fine-tune voice for NAILA
4. **Local TTS on Robot**: Move synthesis to robot for lower latency
5. **Neural Codec Support**: Higher quality audio codecs

---

## References

### Documentation
- [Piper TTS GitHub](https://github.com/rhasspy/piper)
- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)
- [eSpeak-ng Documentation](https://github.com/espeak-ng/espeak-ng)
- [Lessac Voice Dataset](https://www.cstr.ed.ac.uk/projects/blizzard/)

### Related Code
- `services/llm.py` - Similar service pattern to follow
- `agents/response_generator.py` - Where text responses are generated
- `mqtt/handlers/ai_handlers.py` - Where audio will be published
- `config/hardware_config.py` - Hardware optimization
- `server/lifecycle.py` - Server startup/shutdown

---

## Status Tracking

### Phase 1: Service Foundation
- [ ] Create `services/tts.py`
- [ ] Create `config/tts.py`
- [ ] Implement `TTSService` class
- [ ] Implement `load_model()` method
- [ ] Implement `synthesize()` method
- [ ] Hardware optimization configured
- [ ] Basic testing complete

### Phase 2: Audio Synthesis Engine
- [ ] Text normalization implemented
- [ ] Phoneme conversion working
- [ ] ONNX inference running
- [ ] Audio post-processing
- [ ] Prosody control tuned

### Phase 3: Output Format Handling
- [ ] Multiple format support
- [ ] Audio encoding (MP3/OGG)
- [ ] MQTT message formatting
- [ ] Compression working
- [ ] Format validation

### Phase 4: Response Pipeline Integration
- [ ] Integrate with response generator
- [ ] MQTT audio publishing
- [ ] Text and audio sent together
- [ ] Configuration options
- [ ] End-to-end flow working

### Phase 5: Server Lifecycle Integration
- [ ] Add TTS loading phase to startup
- [ ] Integrate into `lifecycle.py`
- [ ] Implement shutdown handling
- [ ] Error handling for load failures
- [ ] Status logging implemented

### Phase 6: Performance & Monitoring
- [ ] Synthesis metrics tracked
- [ ] Health monitoring integration
- [ ] Performance logging
- [ ] Optimization applied
- [ ] Metrics validated

### Phase 7: Advanced Features
- [ ] SSML support implemented
- [ ] Emotion/tone control
- [ ] Multi-voice support
- [ ] Audio effects applied

### Phase 8: Testing & Validation
- [ ] Unit tests written and passing
- [ ] Integration tests written and passing
- [ ] Audio quality validated
- [ ] Manual testing complete
- [ ] Quality meets expectations

---

**Last Updated**: 2025-11-04
**Next Review**: After Phase 1 completion
