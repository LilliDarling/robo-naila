# NAILA AI Orchestration Server

This directory contains the Python-based AI orchestration system that acts as the distributed "brain" for the NAILA robot ecosystem. It implements a sophisticated multi-agent architecture handling AI processing, personality management, memory systems, and device coordination across multiple robot types.

The system is designed around autonomous agents that collaborate to provide intelligent, contextual, and personalized interactions while maintaining complete privacy through local processing.

## ⚠️ Important Note: Robot Firmware

This server works in conjunction with the robot's firmware. Ensure that the firmware is flashed onto your ESP32-S3 Sense board and configured to connect to this server.

Refer to the firmware's documentation for setup details:
[Firmware](../firmware/README.md)

## Project Structure

* **`main.py`**: The main entry point for the server application. It initializes the communication manager and AI services.
* **`config.py`**: Centralized configuration file for server settings (e.g., port numbers, model paths, API keys for any external services if integrated).
* **`requirements.txt`**: Lists all Python dependencies required by the server. These are managed by `uv` (recommended).
* **`uv.lock`**: (Generated by `uv`) Locks the exact versions of all Python dependencies for reproducible environments.
* **`services/`**: Contains modular implementations of various AI functionalities:
    * `stt_service.py`: Handles Speech-to-Text (ASR) using models like [Whisper](https://github.com/openai/whisper) (via `faster-whisper` or `whisper-cpp-python`).
    * `nlu_llm_service.py`: Manages Natural Language Understanding and Large Language Model inference using local LLMs (e.g., Llama 3 via `llama.cpp` integration).
    * `tts_service.py`: Converts text responses into speech audio.
    * `vision_service.py`: For more complex vision analysis offloaded from the robot (e.g., advanced facial recognition, object detection).
* **`core/`**: Contains the central logic for the robot's "brain":
    * `robot_brain.py`: Orchestrates the overall conversational flow, state management, and decision-making based on AI service outputs.
    * `communications_manager.py`: Handles the actual network communication with the robot's embedded client (e.g., MQTT, WebSockets, HTTP).
* **`models/`**: Dedicated directory for storing the large AI model files.
    * `stt/`: STT model files (e.g., Whisper `.gguf` files).
    * `llm/`: LLM model files (e.g., Llama 3 `.gguf` files).
    * `tts/`: TTS model files (e.g., Llama-OuteTTS `.gguf` files).
    * `vision/`: Vision model files.
* **`update_server/`**: Contains a simple HTTP server to host firmware `.bin` files for Over-The-Air (OTA) updates to the robot.
    * `ota_http_server.py`: The script to run this local web server.
    * `firmware_bins/`: Directory where the firmware `.bin` files are stored, typically organized by version.

## Setup Instructions

These instructions assume you have `uv` installed. If not, follow the `uv` installation guide: [uv Install](https://docs.astral.sh/uv/getting-started/installation/)

1.  **Clone the repository:**
    If you haven't already cloned the entire `robo_naila` repository, do so:
    ```bash
    git clone --recursive https://github.com/LilliDarling/robo_naila
    cd robo_naila/ai-server
    ```

2.  **Create and activate a Python virtual environment:**
    It's highly recommended to use a virtual environment to manage dependencies for the server.
    ```bash
    uv venv
    source .venv/bin/activate # On Linux/macOS
    # Or for Windows: .\.venv\Scripts\activate
    ```

3.  **Install Python dependencies:**
    Install all required Python packages using `uv`. This will use `requirements.txt` and generate/use `uv.lock`.
    ```bash
    uv pip install -r requirements.txt
    ```

4.  **Download AI Models:**
    The AI services rely on large pre-trained models. You will need to manually download these into the `models/` subdirectory.

    * **Whisper (STT) - via `faster-whisper`:**
        * Download a quantized `.gguf` model (e.g., `tiny.en`, `base.en`). These can be found on Hugging Face (search for "Systran/faster-whisper-base" for `.gguf` versions).
        * **Recommended:** Start with `tiny.en` for quick testing, then try `base.en` for better accuracy.
        * Place the downloaded `.gguf` file into `models/stt/`.
        * *Example:* `models/stt/whisper-tiny.en.gguf`

    * **Local LLM (NLU/LLM) - via `llama-cpp-python`:**
        * Download a quantized `.gguf` version of a suitable Large Language Model. Llama 3 8B Instruct is a good balance of size and performance for local inference. Look for `.gguf` files on Hugging Face (e.g., from `TheBloke` or `Qwen` for Llama-3-8B-Instruct-GGUF).
        * **Recommended Quantization:** Q4_K_M or Q5_K_M offers good performance and reasonable file size.
        * Place the downloaded `.gguf` file into `models/llm/`.
        * *Example:* `models/llm/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf`

    * **TTS Model - `Llama-OuteTTS` via `llama-cpp-python`:**
        * The `Llama-OuteTTS-1.0-1B-GGUF` model is designed to be run using `llama.cpp` or its Python bindings `llama-cpp-python`.
        * Download the `.gguf` model file from its Hugging Face repository: [OuteAI/Llama-OuteTTS-1.0-1B-GGUF](https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF)
        * **Note:** This model often requires a speaker reference audio file (e.g., a 10-second WAV file) to perform voice cloning. You will need to provide such a file when using the TTS service.
        * Place the downloaded `.gguf` file into `models/tts/`.
        * *Example:* `models/tts/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf` (or similar quantization).

    * **Vision Model - `Ultralytics YOLOv8`:**
        * For real-time object detection, download a lightweight YOLOv8 model.
        * Download `yolov8n.pt` (PyTorch format, can be used directly by `ultralytics` package) or `yolov8n.onnx` from the [Ultralytics GitHub releases](https://github.com/ultralytics/ultralytics/releases).
        * Place the downloaded model file into `models/vision/`.
        * *Example:* `models/vision/yolov8n.pt`

    * **Important:** Update `config.py` to correctly point to the paths of your downloaded models.

5.  **Configure `config.py`:**
    Open `config.py` and adjust any settings as needed, such as:
    * `MQTT_BROKER_HOST`: The IP address of the MQTT broker (often `localhost` if running on the same machine as the server, or the server's LAN IP).
    * `MQTT_BROKER_PORT`: The port for the MQTT broker (default is usually `1883`).
    * `OTA_SERVER_PORT`: The port for the local OTA HTTP server.
    * `STT_MODEL_PATH`: Path to your downloaded Whisper model (e.g., `"models/stt/whisper-tiny.en.gguf"`).
    * `LLM_MODEL_PATH`: Path to your downloaded LLM (e.g., `"models/llm/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"`).
    * `TTS_MODEL_PATH`: Path to your downloaded Llama-OuteTTS GGUF file (e.g., `"models/tts/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf"`).
    * `TTS_SPEAKER_REFERENCE_PATH`: (New) Path to a WAV file for speaker reference for Llama-OuteTTS (e.g., `"models/tts/my_speaker_ref.wav"`).
    * `VISION_MODEL_PATH`: Path to your downloaded Vision model (if applicable, e.g., `"models/vision/yolov8n.pt"`).

## Running the Server

1.  **Ensure your virtual environment is active:**
    ```bash
    source .venv/bin/activate # On Linux/macOS
    # Or for Windows: .\.venv\Scripts\activate
    ```

2.  **Start the main AI server:**
    ```bash
    python main.py
    ```
    The server will start listening for connections from the robot and initializing the AI models. You should see log messages indicating that the services are starting up.

3.  **Start the OTA Update Server (in a separate terminal):**
    The OTA server needs to run concurrently to serve firmware files.
    * Open a new terminal.
    * Navigate back to the `server/` directory.
    * Activate the virtual environment in this new terminal.
    * Run the OTA server script:
        ```bash
        cd robo_naila/ai-server # if not already there
        source .venv/bin/activate
        python infrastructure/ota_http_server.py
        ```
        This server will run on the port specified in `config.py` (e.g., `8070` or `8071`).

## Communication Protocol

The server communicates with the robot firmware primarily using **MQTT (Message Queuing Telemetry Transport)**.

* **MQTT Broker:** The server connects to a local MQTT broker (e.g., Mosquitto), and the robot firmware will also connect to this same broker.
* **Topics:** Communication happens over specific MQTT topics.
    * **Robot to Server (Publish):**
        * `robot/<robot_id>/audio`: For streaming audio chunks after wake word detection.
        * `robot/<robot_id>/vision`: For sending image snippets or vision events.
        * `robot/<robot_id>/status`: For sending heartbeat, battery level, sensor data, etc.
        * `robot/<robot_id>/command_ready`: To signal the robot is ready for the next command/response.
    * **Server to Robot (Publish):**
        * `robot/<robot_id>/response_audio`: For sending TTS audio data.
        * `robot/<robot_id>/action_command`: For sending specific motor or display commands (e.g., "move head left," "show happy expression").
        * `robot/<robot_id>/ota_notification`: To notify the robot of an available firmware update.

A detailed communication protocol specification, including precise topic structures and message payloads, will be documented in `docs/communication_protocol.md`.

## Custom AI Models

This server is designed to allow easy swapping of AI models. Once you have built your own custom STT, NLU/LLM, or TTS models, you can:

1.  Ensure they are converted to a compatible format (e.g., `.gguf` for LLMs, ONNX, or specific library formats).
2.  Place the model files in the appropriate `models/` subdirectory.
3.  Update the model paths in `config.py` to point to your new models.
4.  Restart the server.
